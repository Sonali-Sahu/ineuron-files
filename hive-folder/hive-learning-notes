# check databases list
show databases;

# create database 
create database my_db;

#use hive db
use my_db;

#to check DWH location in HDFS (hadoop)
abc@994915bf825e:~/workspace$ hadoop fs -ls /
Found 2 items
drwx-wx-wx   - abc supergroup          0 2023-02-26 13:42 /tmp
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user
abc@994915bf825e:~/workspace$ hadoop fs -ls /user
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user/hive
abc@994915bf825e:~/workspace$ hadoop fs -ls /user/hive
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user/hive/warehouse
abc@994915bf825e:~/workspace$ hadoop fs -ls /user/hive/warehouse
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user/hive/warehouse/my_db.db


# command to put local csv file into hdfs 
abc@994915bf825e:~/workspace$ hadoop fs -ls /
Found 2 items
drwx-wx-wx   - abc supergroup          0 2023-02-26 13:42 /tmp
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user
abc@994915bf825e:~/workspace$ hadoop fs -put depart_data.csv /tmp
2023-02-26 14:21:27,489 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
abc@994915bf825e:~/workspace$ ls
depart_data.csv  README.md
abc@994915bf825e:~/workspace$ pwd
/config/workspace
abc@994915bf825e:~/workspace$ hadoop fs -ls /tmp
Found 2 items
-rw-r--r--   1 abc supergroup        655 2023-02-26 14:21 /tmp/depart_data.csv
drwx-wx-wx   - abc supergroup          0 2023-02-26 13:43 /tmp/hive

abc@994915bf825e:~/workspace$ hadoop fs -ls /user/hive/
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 13:46 /user/hive/warehouse
abc@994915bf825e:~/workspace$ hadoop fs -ls /user/hive/warehouse
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 14:31 /user/hive/warehouse/my_db.db
abc@994915bf825e:~/workspace$ hadoop fs -ls /user/hive/warehouse/my_db.db
Found 1 items
drwxr-xr-x   - abc supergroup          0 2023-02-26 14:31 /user/hive/warehouse/my_db.db/department_data


# hive commands 
hive> create table department_data
    > (
    > dept_id int,
    > dept_name string,
    > manager_id int,
    > salary int
    > )
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.577 seconds
hive> show tables;
OK
department_data
Time taken: 0.03 seconds, Fetched: 1 row(s)
hive> describe department_data;
OK
dept_id                 int                                         
dept_name               string                                      
manager_id              int                                         
salary                  int                                         
Time taken: 0.268 seconds, Fetched: 4 row(s)
hive> describe formatted department_data;
OK
# col_name              data_type               comment             
dept_id                 int                                         
dept_name               string                                      
manager_id              int                                         
salary                  int                                         
                 
# Detailed Table Information             
Database:               my_db                    
OwnerType:              USER                     
Owner:                  abc                      
CreateTime:             Sun Feb 26 14:31:23 IST 2023     
LastAccessTime:         UNKNOWN                  
Retention:              0                        
Location:               hdfs://localhost/user/hive/warehouse/my_db.db/department_data    
Table Type:             MANAGED_TABLE            
Table Parameters:                
        COLUMN_STATS_ACCURATE   {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"dept_id\":\"true\",\"dept_name\":\"true\",\"manager_id\":\"true\",\"salary\":\"true\"}}
        bucketing_version       2                   
        numFiles                0                   
        numRows                 0                   
        rawDataSize             0                   
        totalSize               0                   
        transient_lastDdlTime   1677402083          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             ,                   
        serialization.format    ,                   
Time taken: 0.236 seconds, Fetched: 35 row(s)

PS C:\Users\sonal> cd C:\docker_folder\HiveDockerSetup
PS C:\docker_folder\HiveDockerSetup> docker cp AgentLogingReport.csv namenode:AgentLogingReport.csv
PS C:\docker_folder\HiveDockerSetup> docker exec -it namenode bash
root@7ae262842071:/# hdfs dfs -mkdir -p /data/
root@7ae262842071:/# hdfs dfs -put AgentLogingReport.csv /data/AgentLogingReport.csv
2023-04-22 12:42:03,800 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
root@7ae262842071:/# ls
AgentLogingReport.csv  bin   dev            etc     hadoop-data  lib    media  opt   root  run.sh  srv  tmp  var
KEYS                   boot  entrypoint.sh  hadoop  home         lib64  mnt    proc  run   sbin    sys  usr
root@7ae262842071:/# exit
exit
PS C:\docker_folder\HiveDockerSetup> docker-compose down
















































































